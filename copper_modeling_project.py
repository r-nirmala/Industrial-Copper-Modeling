# -*- coding: utf-8 -*-
"""Copper_modeling_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12CUyV4o3vCxXA9F4zD210bBARmZLLsCR
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from datetime import datetime,timedelta
from sklearn.preprocessing import OrdinalEncoder
import seaborn as sns
import matplotlib.pyplot as plt

#read file from github
df = pd.read_excel(r"https://github.com/r-nirmala/Industrial-Copper-Modeling/raw/main/Data/Copper_Set.xlsx")
df.sample(3)

df['width'].max()

#counts unique values for each column
for i in df.columns:
    print(i,':', df[i].nunique())

#to see null values and datatypes
df.info()

#count null values
df.isnull().sum()

df.columns

#convert datatypes and date-time format
df['quantity tons'] = pd.to_numeric(df['quantity tons'],errors='coerce')
df['item_date_1'] = pd.to_datetime(df['item_date'],format='%Y%m%d',errors='coerce').dt.date
df['delivery_date_1'] = pd.to_datetime(df['delivery date'],format='%Y%m%d',errors='coerce').dt.date
df.sample(3)

#making material ref column's unwanted values to nan
df['material_ref']=df['material_ref'].apply(lambda x: np.nan if str(x).startswith('0000') else x)
df.head(3)

df.isnull().sum()

#materi ref column has more null values. remove that column and id column
df.drop(columns=['id','material_ref'],inplace=True)
df.sample(3)

#descriptive statistics of df
df.describe().T

#quantity tons and selling price values below 0. Convert them to null.
df['quantity tons']=df['quantity tons'].apply(lambda x: np.nan if (x<=0) else x)
df['selling_price']=df['selling_price'].apply(lambda x: np.nan if (x<=0) else x)
df.describe().T

df.isnull().sum()

df.dtypes

#handle null values using median,mode
#median - middle value in dataset, mode - value that appears most frequently in dataset

# object datatype using mode
df['item_date'].fillna(df['item_date'].mode().iloc[0],inplace=True)
df['item_date_1'].fillna(df['item_date_1'].mode().iloc[0],inplace=True)
df['status'].fillna(df['status'].mode().iloc[0],inplace=True)
df['delivery date'].fillna(df['delivery date'].mode().iloc[0],inplace=True)
df['delivery_date_1'].fillna(df['delivery_date_1'].mode().iloc[0],inplace=True)

#numerical datatype using median
df['quantity tons'].fillna(df['quantity tons'].median(), inplace=True)
df['customer'].fillna(df['customer'].median(), inplace=True)
df['country'].fillna(df['country'].median(), inplace=True)
df['application'].fillna(df['application'].median(), inplace=True)
df['thickness'].fillna(df['thickness'].median(), inplace=True)
df['selling_price'].fillna(df['selling_price'].median(), inplace=True)

df.isnull().sum()

df['status'].unique()

df['item type'].unique()

# convert categorical data into numerical data - using map and ordinal encoder methods

df['status'] = df['status'].map({'Lost':0, 'Won':1, 'Draft':2, 'To be approved':3, 'Not lost for AM':4,
                                 'Wonderful':5, 'Revised':6, 'Offered':7, 'Offerable':8})
df['item type'] = OrdinalEncoder().fit_transform(df[['item type']])
df

df['status'].unique()

df['item type'].unique()

#check for null and describe function finally
df.isnull().sum()

df.describe().T

"""**Skewness Handling - Log Transformation**"""

#detecting skewness in columns using plot
def plot(df,column):
  #distplot
  plt.figure(figsize=(13,3))
  plt.subplot(1,3,1)
  sns.distplot(df[column])
  plt.title("distplot for"+" "+column)

  #histogram plot

  plt.subplot(1,3,2)
  sns.histplot(df, x= column, kde= True, bins=30,color="salmon")
  plt.title("histogram plot for"+" "+column)

  #boxplot

  plt.subplot(1,3,3)
  sns.boxplot(df, x=column)
  plt.title("Box plot for"+" "+column)

columns=['quantity tons', 'customer', 'country', 'status',
                'item type', 'application', 'thickness', 'width', 'product_ref',
                'selling_price']

for i in columns:
  plot(df,i)

#Skewed columns - quantity tons,customer,thickness,selling_price
#use log transformation method to reduce skewness
df1 = df.copy()

df1['quantity_tons_log']=np.log(df['quantity tons'])
df1['customer_log']=np.log(df['customer'])
df1['thickness_log']=np.log(df['thickness'])
df1['selling_price_log']=np.log(df['selling_price'])

df1.columns

#check skewness after log trans. by plots
columns_log = ['quantity_tons_log', 'customer_log', 'thickness_log','selling_price_log']
for i in columns_log:
  plot(df1,i)

"""**Outliers Handling - Interquartile Range(IQR) Method**"""

df2 = df1.copy()
df2.head()

# Using IQR and clip() methods to handle the outliers
def outlier(df,column):
  q1= df[column].quantile(0.25)
  q3= df[column].quantile(0.75)

  iqr= q3-q1

  upper_threshold= q3 + (1.5*iqr)
  lower_threshold= q1 - (1.5*iqr)

  df[column]= df[column].clip(lower_threshold, upper_threshold)

# (Ex: lower threshold = 5 and upper threshold = 20)
# above upper threshold values (>20) are converted to upper threshold value (20) in features
# below lower threshold values (<5)  are converted to lower threshold value (5)  in features

df2.describe().T

df2.columns

outlier_columns= ['quantity_tons_log', 'customer_log', 'thickness_log','selling_price_log','width','application']
for i in outlier_columns:
  outlier(df2,i)

df2.describe().T

for i in outlier_columns:
  plot(df2,i)

# Droping the unwanted columns - quantity tons,customer,thickness,selling_price,
# since we have log columns of all these.

df3= df2.drop(columns=["quantity tons","customer","thickness","selling_price"])
df3.head()

#check the data types
df3.dtypes

# Verification of highly correlated columns using Heatmap. If any columns correlation value >= 0.7 (absolute value), drop the columns.
corr = df3.drop(columns=['item_date','delivery date','item_date_1','delivery_date_1']).corr()
plt.figure(figsize=(8,3))
sns.heatmap(corr, annot= True, fmt="0.2f")
plt.show()

"""The highest value is (0.4 or -0.42). So, no columns are highly correlated and no need to drop any columns.

# Wrong Delivery date Handling
"""

#In data, delivery date precedes item date. It should be changed
df4 = df3.copy()
df4.head()

df4.dtypes

#converting object datatypes to datetime
df4['item_date_1']=pd.to_datetime(df4['item_date_1'])
df4['delivery_date_1']=pd.to_datetime(df4['delivery_date_1'])

df4.columns

#identify date difference bet. delivery date and item date
df4['date_differ']=(df4['delivery_date_1'] - df4['item_date_1']).dt.days
df4.head()

#-ve values in date differ. indicates delivery date precedes item date. So, we predict correct delivery date using ML algorithms.
#So,split item date_1 into day,month,year for prediction.
df4['item_date_day']=df4['item_date_1'].dt.day
df4['item_date_month']=df4['item_date_1'].dt.month
df4['item_date_year']=df4['item_date_1'].dt.year
df4.sample(3)

#seperating +ve value df and _ve value df based on date differ column
df4_pv = df4[df4['date_differ']>=0]
df4_pv.reset_index(drop=True,inplace=True)
df4_pv.sample(3)

df4_nv= df4[df4['date_differ']<0]
df4_nv.reset_index(drop=True,inplace=True)
df4_nv.sample(3)

"""### Train the model with df4_pv(correct deliv.dates) dataframe and predict date difference for df4_nv(incorrect deliv.dates) dataframe"""

from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from datetime import datetime,timedelta

df4_pv.columns

#find the best algorithm
def delivery_date(df, algorithm):
  x = df.drop(columns=['item_date_1', 'delivery_date_1','date_differ'],axis=1)
  y = df['date_differ']
  x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

  model=algorithm().fit(x_train,y_train)
  y_pred = model.predict(x_test)

  mse = mean_squared_error(y_test, y_pred)
  rmse = np.sqrt(mse)
  r2 = r2_score(y_test, y_pred)
  mae = mean_absolute_error(y_test, y_pred)

  metrics = {'Algorithm': str(algorithm).split("'")[1].split(".")[-1],
              'R2': r2,
              'Mean Absolute Error': mae,
              'Mean Squared Error': mse,
              'Root Mean Squared Error': rmse}

  return metrics

print(delivery_date(df4_pv, DecisionTreeRegressor))
print(delivery_date(df4_pv, ExtraTreesRegressor))
print(delivery_date(df4_pv, RandomForestRegressor))
print(delivery_date(df4_pv, AdaBoostRegressor))
print(delivery_date(df4_pv, GradientBoostingRegressor))
print(delivery_date(df4_pv, XGBRegressor))

df4_pv.columns

df4_nv.columns

# Random Forest algorithm is low bias and reduce overfitting compared to others.
# Train the model with random forest algorithm

def RandomForest(train_df, test_df):
  #df4_pv data for training
  x= train_df.drop(columns=["item_date_1", "delivery_date_1", "date_differ"], axis=1)
  y= train_df["date_differ"]

  x_train, x_test, y_train, y_test= train_test_split(x,y, test_size= 0.2, random_state=42)
  model= RandomForestRegressor().fit(x_train, y_train)

  #df4_nv data for prediction
  data= test_df.drop(columns=["item_date_1", "delivery_date_1", "date_differ"],axis=1)

  y_pred=model.predict(data)

  return y_pred

date_difference = RandomForest(df4_pv, df4_nv)
date_difference

# convert float values into integer using list comprehension method
date_difference_1 = [int(round(i,0)) for i in date_difference]

#add this predicted date difference column to df4_nv dataframe
df4_nv['date_differ'] = pd.DataFrame(date_difference_1)
df4_nv

# calculate delivery date (item_date + Date_difference = delivery_date)

def find_delivery_date(item_date, date_difference):
    result_date = item_date + timedelta(days=date_difference)
    delivery_date = result_date.strftime("%Y-%m-%d")
    return delivery_date

# find out the delivery date and add to dataframe

df4_nv['item_date_1'] = pd.to_datetime(df4_nv['item_date_1'])
df4_nv['delivery_date_1'] = df4_nv.apply(lambda x: find_delivery_date(x['item_date_1'], x['date_differ']), axis=1)
df4_nv

#concatenate two dataframes - df4_pv and df4_nv
df_final = pd.concat([df4_pv,df4_nv], axis=0, ignore_index=True)
df_final.tail(3)

# split the day, month and year from 'delivery_date_1' column - useful for prediction.
df_final['delivery_date_1']= pd.to_datetime(df_final['delivery_date_1'])

df_final['delivery_date_day']= df_final['delivery_date_1'].dt.day
df_final['delivery_date_month']= df_final['delivery_date_1'].dt.month
df_final['delivery_date_year']= df_final['delivery_date_1'].dt.year

#drop the unwanted columns
df_final.drop(columns = ['item_date','delivery date', 'item_date_1', 'delivery_date_1','date_differ'], axis=1, inplace=True)
df_final.sample(3)

#convert to csv file
df_final.to_csv('Copper_data_final', index=False)

"""### **Classification Algorithm to Predict Status(Won/Lost)**"""

from imblearn.combine import SMOTETomek
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc,accuracy_score
import matplotlib.pyplot as plt
import pickle

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

df_final = pd.read_csv('https://github.com/r-nirmala/Industrial-Copper-Modeling/raw/main/Data/Copper_data_final')
df_final.head()

df_final.dtypes

df_c = df_final.copy()

#filter status column values with 1[Won] and 0[Lost] in new df.
df_c = df_c[(df_c.status == 1) |(df_c.status == 0)]
df_c.sample(4)

df_c['status'].value_counts()

#Do oversampling method to balance the datas in status column.

x= df_c.drop('status',axis=1)
y= df_c['status']

x_new, y_new = SMOTETomek(). fit_resample(x,y)

x.shape, y.shape

x_new.shape, y_new.shape

y_new.value_counts()

#status column values are balanced now. So, we continue for ML prediction
#finding the best algorithm

def classification_algorithm(x_new, y_new, algorithm):
  x_train, x_test, y_train, y_test =  train_test_split(x_new, y_new, train_size=0.2,random_state=42)
  model = algorithm().fit(x_train, y_train)

  y_pred_train= model.predict(x_train)
  y_pred_test = model.predict(x_test)

  accuracy_train= accuracy_score(y_train, y_pred_train)
  accuracy_test= accuracy_score(y_test, y_pred_test)

  metrics={"Algorithm": algorithm.__name__,
           "Accuracy_Train": accuracy_train,
           "Accuracy_Test": accuracy_test}
  return metrics

#before oversampling results

print(classification_algorithm(x, y,DecisionTreeClassifier))
print(classification_algorithm(x, y,ExtraTreesClassifier))
print(classification_algorithm(x, y,RandomForestClassifier))
print(classification_algorithm(x, y,AdaBoostClassifier))
print(classification_algorithm(x, y,GradientBoostingClassifier))
print(classification_algorithm(x, y,XGBClassifier))

#after oversampling results

print(classification_algorithm(x_new, y_new,DecisionTreeClassifier))
print(classification_algorithm(x_new, y_new,ExtraTreesClassifier))
print(classification_algorithm(x_new, y_new,RandomForestClassifier))
print(classification_algorithm(x_new, y_new,AdaBoostClassifier))
print(classification_algorithm(x_new, y_new,GradientBoostingClassifier))
print(classification_algorithm(x_new, y_new,XGBClassifier))

"""After oversampling there is a good accuracy.

Extratrees classifier and Randomforest classifier have good test accuracy. But, in train accuracy it is overfitting.

Randomforest classifier has good interpretability. So, i choose that.
"""

#Hyperparameter Tuning method using GridsearchCV - reduces overfitting and gives better accuracy.
#manually pass parametrs and get best parameters.

x_train, x_test, y_train, y_test = train_test_split(x_new,y_new,test_size=0.2,random_state=42)

parameters = {'max_depth'        : [2, 5, 10, 20],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf' : [1, 2, 4],
              'max_features'     : ['sqrt', 'log2']}

grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, cv=5, n_jobs=-1)
grid_search.fit(x_train, y_train)

grid_search.best_params_

grid_search.best_score_

# passing the parameters in the random forest algorithm and check the accuracy for training and testing

x_train, x_test, y_train, y_test = train_test_split(x_new,y_new,test_size=0.2,random_state=42)
model = RandomForestClassifier(max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)

y_pred_train = model.predict(x_train)
y_pred_test = model.predict(x_test)

accuracy_train = accuracy_score(y_train, y_pred_train)
accuracy_test = accuracy_score(y_train, y_pred_train)

print("Accuracy score for Train and Test")
print("----------------------------------")
print("Accuracy_Train: ",accuracy_train)
print("Accuracy_Test: ",accuracy_test)
print("  ")

#overfitting in training data is reduced after hyperparameter tuning. Model works well for unseen data

#check accuracy for metrics
x_train, x_test, y_train, y_test = train_test_split(x_new,y_new,test_size=0.2,random_state=42)
model = RandomForestClassifier(max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)

y_pred_test = model.predict(x_test)

print("Confusion_matrix")
print("--------------------------")
print(confusion_matrix(y_true= y_test, y_pred= y_pred_test))
print(" ")
print("Classification_report")
print("-------------------------------")
print(classification_report(y_true= y_test, y_pred= y_pred_test))

# Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)

FP,TP,threshold = roc_curve(y_true=y_test, y_score=y_pred_test)
auc_curve = auc(x=FP, y=TP)
print(auc_curve)

plt.plot(FP, TP, label=f"ROC Curve (area={round(auc_curve, 2)}) ")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.10])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Pass user data manually to check the model's status prediction

user_data = np.array([[30153963, 30, 6, 28, 952, 628377, 5.9, -0.96, 6.46, 1,4,2021,1,1,2021]])
y_pred = model.predict(user_data)
if y_pred[0] == 1:
    print('Won')
else:
    print('Lose')

user_data = np.array([[77.0,3.0,10.0,1500.0,164141591,3.677655,17.222226,0.000000,7.110696,1,4,2021,1,8,2021]])
y_pred = model.predict(user_data)
if y_pred[0] == 1:
    print('Won')
else:
    print('Lose')

#save the model using the pickle

with open("classification_model.pkl","wb") as f:
    pickle.dump(model,f)

df_reg.tail(2)

#load the model and check prediction
with open('/content/drive/MyDrive/Copper_project/classification_model.pkl','rb') as f:
  l_model = pickle.load(f)
user_data = np.array([[25.0,5.0,41.0,1240.0,164141591,6.008043,17.223381,-0.342490,6.408529,2,7,2020,1,8,2020]])
y_p = l_model.predict(user_data)
#if y_p[0] == 1:
 #   print('Won')
#else:
 #   print('Lose')
y_p[0]

"""### **Regression Algorithm to Predict Selling Price**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
import pickle

import warnings
warnings.filterwarnings('ignore')

df_final = pd.read_csv('https://github.com/r-nirmala/Industrial-Copper-Modeling/raw/main/Data/Copper_data_final')
df_final.columns

df_reg = df_final.copy()

df_reg.head(3)

#find the best algorithm by checking r2_score for train and test data
def ML_regression(df, algorithm):
  x = df.drop(columns=['selling_price_log'],axis=1)
  y = df['selling_price_log']

  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
  model = algorithm().fit(x_train, y_train)

  y_pred_train = model.predict(x_train)
  y_pred_test = model.predict(x_test)

  r2_train = r2_score(y_train,y_pred_train)
  r2_test = r2_score(y_test,y_pred_test)

  accuracy_metrics = {
            'algorithm': algorithm.__name__,
            'R2_train': r2_train,
            'R2_test': r2_test}
  return accuracy_metrics

print(ML_regression(df_reg, DecisionTreeRegressor))
print(ML_regression(df_reg, ExtraTreesRegressor))
print(ML_regression(df_reg, RandomForestRegressor))
print(ML_regression(df_reg, AdaBoostRegressor))
print(ML_regression(df_reg, GradientBoostingRegressor))
print(ML_regression(df_reg, XGBRegressor))

#RandomForest algorithm has good testing accuracy than others.
#But, Training accuracy is overfitting.

#Reduce overfitting by hyperparameter tuning with GridSearchCV.
x = df_reg.drop(columns=['selling_price_log'], axis=1)
y = df_reg['selling_price_log']
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)

parameters = {'max_depth'      : [2, 5, 10, 20],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf' : [1, 2, 4],
              'max_features'     : ['sqrt', 'log2', None]}

grid_search_r = GridSearchCV(estimator=RandomForestRegressor(), param_grid=parameters, cv=5, n_jobs=-1)
grid_search_r.fit(x_train, y_train)

grid_search_r.best_params_

grid_search_r.best_score_

#pass these parameters in algorithm and check accuracy of r2_score

x = df_reg.drop(columns=['selling_price_log'],axis=1)
y = df_reg['selling_price_log']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = RandomForestRegressor(max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)

y_pred_train = model.predict(x_train)
y_pred_test = model.predict(x_test)

r2_train = r2_score(y_train,y_pred_train)
r2_test = r2_score(y_test,y_pred_test)


print('R2_train:', r2_train, '\n', 'R2_test:', r2_test)

#overfitting in training data is reduced after hyperparameter tuning

#Use the hypertuned parameters and check metrics accuracy for selling price prediction
x = df_reg.drop(columns=['selling_price_log'],axis=1)
y = df_reg['selling_price_log']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = RandomForestRegressor(max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)

y_pred = model.predict(x_test)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2_score = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

metrics_r = { 'R2_score':r2_score,
             'Mean_squared_error':mse,
              'Mean_absolute_error':mae,
              'Root_mean_squared_error':rmse }

metrics_r

df_reg.head(3)

#Predict selling price by passing user input
user_data = np.array([[28.0,1,5.0,10.0,1500.0,1670798778,3.991779,17.221905,0.693147,1,4,2021,1,7,2021]])
y_pred = model.predict(user_data)
y_pred

# using Inverse Log Transformation(exp) to convert the log value to original scale of the data
np.exp(y_pred[0])

#Save the model in pickle
with open('Regression_model.pkl', 'wb') as f:
  pickle.dump(model, f)



#load the pickle model to check selling price prediction
with open('/content/Regression_model.pkl', 'rb')as f:
  model = pickle.load(f)
y_pred = model.predict(np.array([[28.0,1,5.0,10.0,1950.0,640665,2.798297,17.221905,1.481605,18,3,2021,1,8,2021]]))
print("Price with log:",y_pred[0])
print("Price without log:",np.exp(y_pred[0]))

import sklearn
print(sklearn.__version__)

"""**Streamlit part**"""

#Find the unique values and min, max of columns for setting user input options in streamlit

a = df_reg['country'].unique()
a.sort()
a

b = df_reg['application'].unique()
b.sort()
b

c = df_reg['product_ref'].unique()
c.sort()
c

df_reg['thickness_log'].min()

df_reg['thickness_log'].max()

df_reg['width'].min()

df_reg['width'].max()

df_reg['customer_log'].min()

df_reg['customer_log'].max()

df_reg['quantity_tons_log'].min()

df_reg['quantity_tons_log'].max()

df_reg['selling_price_log'].min()

df_reg['selling_price_log'].max()

df_final['delivery_date'] = pd.to_datetime(
    {
        'year': df_final['delivery_date_year'],
        'month': df_final['delivery_date_month'],
        'day': df_final['delivery_date_day']
    }
)
df_final['delivery_date'].min()

df_final['delivery_date'].max()

df_final['item_date'] = pd.to_datetime(
    {
        'year': df_final['item_date_year'],
        'month': df_final['item_date_month'],
        'day': df_final['item_date_day']
    }
)
df_final['item_date'].min()

df_final['item_date'].max()

pip install streamlit

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Copper_project/Regression_model.pkl'

# Load the file
import pickle

with open(file_path, 'rb') as file:
    model = pickle.load(file)

# Use the loaded model (example)
print(model)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile copper.py
# 
# 
# 
# import streamlit as st
# import numpy as np
# from datetime import date
# import pickle
# 
# #page configuration
# st.set_page_config(page_title='Industrial Copper Modeling')
# 
# st.markdown(f'<h2 style="text-align: center;">Industrial Copper Modeling</h2>',
#                 unsafe_allow_html=True)
# 
# #button style and text style functions
# def style_submit_button():
# 
#   st.markdown("""
#                   <style>
#                   div.stButton > button:first-child {
#                                                       background-color: #367F89;
#                                                       color: white;
#                                                       width: 70%}
#                   </style>
#               """, unsafe_allow_html=True)
# 
# 
# def style_prediction():
# 
#   st.markdown(
#           """
#           <style>
#           .center-text {
#               text-align: center;
#               color: violet
#           }
#           </style>
#           """,
#           unsafe_allow_html=True
#       )
# 
# #user input options
# class options:
# 
#     country_values = [25.0, 26.0, 27.0, 28.0, 30.0, 32.0, 38.0, 39.0, 40.0, 77.0, 78.0,
#                        79.0, 80.0, 84.0, 89.0, 107.0, 113.0]
# 
#     status_values = ['Won', 'Lost', 'Draft', 'To be approved', 'Not lost for AM',
#                     'Wonderful', 'Revised', 'Offered', 'Offerable']
# 
#     status_dict = {'Lost':0, 'Won':1, 'Draft':2, 'To be approved':3, 'Not lost for AM':4,
#                 'Wonderful':5, 'Revised':6, 'Offered':7, 'Offerable':8}
# 
#     item_type_values = ['W', 'WI', 'S', 'PL', 'IPL', 'SLAWR', 'Others']
# 
#     item_type_dict = {'W':5.0, 'WI':6.0, 'S':3.0, 'Others':1.0, 'PL':2.0, 'IPL':0.0, 'SLAWR':4.0}
# 
#     application_values = [2.0, 3.0, 4.0, 5.0, 10.0, 15.0, 19.0, 20.0, 22.0, 25.0,26.0,
#                           27.0, 28.0, 29.0, 38.0, 39.0, 40.0, 41.0, 42.0, 56.0, 58.0, 59.0,
#                           65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 79.0, 87.5]
# 
#     product_ref_values = [611728, 611733, 611993, 628112, 628117, 628377, 640400, 640405,640665,164141591,
#                           164336407, 164337175, 929423819, 1282007633, 1332077137, 1665572032, 1665572374,
#                           1665584320, 1665584642, 1665584662, 1668701376, 1668701698, 1668701718, 1668701725,
#                           1670798778, 1671863738, 1671876026, 1690738206, 1690738219, 1693867550,
#                           1693867563, 1721130331, 1722207579]
# 
# #function for regression and classification
# class prediction:
# 
#     def regression():
# 
#         with st.form('Regression'):
# 
#             col1,col2,col3 = st.columns([0.5,0.1,0.5])
# 
#             with col1:
# 
#                 item_date = st.date_input(label='Item Date', min_value=date(2020,2,7),
#                                         max_value=date(2021,4,1), value=date(2020,2,7))
# 
#                 quantity_log = st.text_input(label='Quantity Tons (Min: -0.3223 & Max: 6.9247)')
# 
#                 country = st.selectbox(label='Country', options=options.country_values)
# 
#                 item_type = st.selectbox(label='Item Type', options=options.item_type_values)
# 
#                 thickness_log = st.text_input(label='Thickness (Min: -1.7147 & Max: 3.2815)')
# 
#                 product_ref = st.selectbox(label='Product Ref', options=options.product_ref_values)
# 
# 
#             with col3:
# 
#                 delivery_date = st.date_input(label='Delivery Date', min_value=date(2020,8,1),
#                                             max_value=date(2022,1,1), value=date(2020,8,1))
# 
#                 customer = st.text_input(label='Customer (Min: 17.2191 & Max: 17.2301)')
# 
#                 status = st.selectbox(label='Status', options=options.status_values)
# 
#                 application = st.selectbox(label='Application', options=options.application_values)
# 
#                 width = st.text_input(label='Width (Min: 700.0 & Max: 1980.0)')
# 
#                 st.write('')
#                 st.write('')
#                 button = st.form_submit_button(label='SUBMIT')
#                 style_submit_button()
# 
#         if button:
#             with open(r'/content/drive/MyDrive/Copper_project/Regression_model.pkl', 'rb') as f:
#                 model = pickle.load(f)
# 
#             user_data = np.array([[customer,
#                                 country,
#                                 options.status_dict[status],
#                                 options.item_type_dict[item_type],
#                                 application,
#                                 width,
#                                 product_ref,
#                                 np.log(float(quantity_log)),
#                                 np.log(float(thickness_log)),
#                                 item_date.day, item_date.month, item_date.year,
#                                 delivery_date.day, delivery_date.month, delivery_date.year]])
# 
#             y_pred = model.predict(user_data)
#             selling_price = np.exp(y_pred[0])
#             selling_price = round(selling_price, 2)
#             return selling_price
# 
#     def classification():
# 
#         with st.form('Classification'):
# 
#             col1,col2,col3 = st.columns([0.5,0.1,0.5])
# 
#             with col1:
# 
#                 item_date = st.date_input(label='Item Date', min_value=date(2020,2,7),
#                                         max_value=date(2021,4,1), value=date(2020,2,7))
# 
#                 quantity_log = st.text_input(label='Quantity Tons (Min: -0.3223 & Max: 6.9247)')
# 
#                 country = st.selectbox(label='Country', options=options.country_values)
# 
#                 item_type = st.selectbox(label='Item Type', options=options.item_type_values)
# 
#                 thickness_log = st.text_input(label='Thickness (Min: -1.7147 & Max: 3.2815)')
# 
#                 product_ref = st.selectbox(label='Product Ref', options=options.product_ref_values)
# 
# 
#             with col3:
# 
#                 delivery_date = st.date_input(label='Delivery Date', min_value=date(2020,8,1),
#                                             max_value=date(2022,1,1), value=date(2020,8,1))
# 
#                 customer = st.text_input(label='Customer (Min: 17.2191 & Max: 17.2301)')
# 
#                 selling_price_log = st.text_input(label='Selling Price (Min: 5.9750 & Max: 7.3903)')
# 
#                 application = st.selectbox(label='Application', options=options.application_values)
# 
#                 width = st.text_input(label='Width (Min: 700.0 & Max: 1980.0)')
# 
#                 st.write('')
#                 st.write('')
#                 button = st.form_submit_button(label='SUBMIT')
#                 style_submit_button()
# 
#         if button:
# 
#             with open(r'/content/drive/MyDrive/Copper_project/classification_model.pkl', 'rb') as f:
#                 model = pickle.load(f)
#             user_data = np.array([[customer,
#                                 country,
#                                 options.item_type_dict[item_type],
#                                 application,
#                                 width,
#                                 product_ref,
#                                 np.log(float(quantity_log)),
#                                 np.log(float(thickness_log)),
#                                 np.log(float(selling_price_log)),
#                                 item_date.day, item_date.month, item_date.year,
#                                 delivery_date.day, delivery_date.month, delivery_date.year]])
# 
#             y_pred = model.predict(user_data)
#             status = y_pred[0]
#             return status
# 
# #main streamlit page
# tab1, tab2 = st.tabs(['PREDICT SELLING PRICE', 'PREDICT STATUS'])
# 
# with tab1:
# 
#     try:
#         selling_price = prediction.regression()
#         if selling_price:
#             style_prediction()
#             st.markdown(f'### <div class="center-text">Predicted Selling Price = {selling_price}</div>', unsafe_allow_html=True)
# 
#     except ValueError:
#         col1,col2,col3 = st.columns([0.26,0.55,0.26])
#         with col2:
#             st.warning('##### Quantity Tons / Customer ID is empty')
# 
# with tab2:
# 
#     try:
#         status = prediction.classification()
#         if status == 1:
#             style_prediction()
#             st.markdown(f'### <div class="center-text">Predicted Status = Won</div>', unsafe_allow_html=True)
# 
#         elif status == 0:
#             style_prediction()
#             st.markdown(f'### <div class="center-text">Predicted Status = Lost</div>', unsafe_allow_html=True)
# 
#     except ValueError:
# 
#         col1,col2,col3 = st.columns([0.15,0.70,0.15])
# 
#         with col2:
#             st.warning('##### Quantity Tons / Customer ID is empty')
# 
# 
# 
# 
# 
# 
#

!wget -q -O - ipv4.icanhazip.com

! streamlit run copper.py & npx localtunnel --port 8501